---
title: Data visualization DEAD Data set
format:
  typst
editor_options: 
  chunk_output_type: console
---

```{r}
#| echo: false
#| message: false
#| warning: false

library(ggpubr)
library(tidyverse)
library(janitor)
library(data.table)
library(skimr)
library(stringi)
library(jsonlite)

```

### 

```{r}
#| echo: false
#| message: false
#| warning: false
theme_set(theme_minimal())
theme_update(
  panel.grid.minor = element_blank(),
  panel.grid.major = element_blank(),
  axis.line.x = element_line(color = "grey80", linewidth = .4),
  axis.ticks.x = element_line(color = "grey80", linewidth = .4),
  axis.title.y = element_blank(),
  plot.margin = margin(10, 15, 10, 15)
)
```

```{r}
#| echo: false
#| message: false

#| warning: false
customers <- read_csv("data/customers.csv") |> clean_names()
orders <- read_csv("data/orders.csv") |> clean_names() 
products <- read_csv("data/products.csv") |> clean_names() 
regions <- read_csv("data/regions.csv") |> clean_names()
```

## Exploratory Visualizations: Data visualization in Data Science Course

**Group:** 38

**Data:** DEAD data set

## About the Dumbledilpâ€™s Equipment & Adventuring Distributions (DEAD) Data set

This company has four data sets of type table holding data on their customers, on the orders, products and regions. In total there are `r formatC((nrow(customers)), big.mark =",")` customers and `r formatC(nrow(orders), big.mark = ",")` orders during the 5-year period similarly, there are `r length(unique(regions$territory))` territories.

```{r}
#| echo: false
df_orders_wide <- orders |> 
  # Check of products and quantities match
  mutate(prod_len = str_count(products, ";"),
         quant_len = str_count(quantities, ","), .before = 1) |> 
  # Use only matching rows with equal length of products and quantity
  filter(prod_len == quant_len) |> 
  # Replace delimiter to ensure easy unpacking of data
  mutate(quantities = str_replace_all(quantities, ",", ";"),
         product_code = str_replace_all(products_i_ds, ",", ";"), .before = 1) |> 
  # separate_longer_delim(cols = quantities, delim = ",") |> 
  # Unpack product id, products, quantity
  separate_longer_delim(cols = c(products, quantities, product_code), delim = ";") |> 
  # Create a column for the prices of the different monetary units
  separate_wider_delim(cart_price, delim = ";", names = c("price_gold", "price_silver", "price_copper"))
```

```{r}
customers
```

```{r}
#| echo: false
df_join <- df_orders_wide |> 
  select(!territory) |> 
  # separate_wider_delim(cart_price, delim =  "; ", names = c("gold", "silver", "copper")) |> 
  mutate(gold = parse_number(price_gold),
         silver = parse_number(price_silver),
         copper = parse_number(price_copper)) |> 
  # Join orders to customer table
  inner_join(customers |> 
               rename(customer_id = account_code), join_by(customer_id)) |> 
  # Join regions to customer/oder data 
  inner_join(regions, join_by(territory)) |>
  mutate(product_code = as.integer(product_code)) |> 
  # Join products to the rest of the dataset
  inner_join(products , join_by(product_code)) |> 
  
  mutate(
    
    delay = delivery_date - order_date,
    monthly_date =  floor_date(   # make new column, week of onset
    order_date,
    unit = "month")) |> 
  mutate(
    year = lubridate::year(order_date),
    week = lubridate::week(order_date),
    month = lubridate::month(order_date),
    month_name = lubridate::month(order_date, label = TRUE)
  ) |> 
  mutate(across(where(is.character), str_to_sentence))
  
```

```{r}
#| echo: false
#| warning: false
#| message: false
#| label: fig-cust
#| fig-cap: Customer account details
#| fig-subcap: 
#|  - "Account type"
#|  - "Account name"
#| layout-ncol: 2
#| fig-height: 4.5
customers |>
  mutate(key_account = key_account |> fct_infreq() |> fct_rev()) |>
  ggplot(aes(x = key_account)) +
  geom_bar() +
  coord_flip() +
  labs(
    x = NULL,
    y = "Frequency"
  ) 

df_join |> 
  mutate(account_type = stri_trans_totitle(account_type, type = "sentence") |> fct_infreq() |> fct_rev() ) |> 
  ggplot(aes(x = account_type)) +
  labs(
    x = NULL,
    y = "Frequency"
  ) +
  geom_bar() + 
  coord_flip()


```

The account type and account names are all categorical nominal variables. There are three levels of account type, with No key account being the most common. Further, there are `r length(unique(customers$account_type))`levels of account type

```{r}
#| echo: false
#| warning: false
#| message: false
#| fig-cap: Distribution of delivery delay and cart price by year
#| label: fig-cont
#| fig-subcap: 
#|  - "Delivery delay"
#|  - "Cart price"
#| layout-ncol: 2
#| fig-height: 4.5
orders |> 
  mutate(yea_transact = factor(year(order_date)),
         m_transact = month(order_date),
         delay = delivery_date - order_date) |> 
  ggplot(aes(x= delay)) +
  geom_histogram(bins = 10) +
  labs(x = "Delivery delay (days)") +
  facet_wrap(~yea_transact) 
orders |> 
  mutate(yea_transact = factor(year(order_date))) |> 
  ggplot(aes(x= cart_price_in_cp)) +
  geom_histogram(bins = 15) +
  facet_wrap(~yea_transact) +
  labs(x = "Cart price in copper")
```

There distribution of cart price @fig-cont-1 and delivery delay @fig-cont-2 are fairly similar across the five years of transactions. There are few cases where the deliver delay was more than 100 days.

```{r}
#| echo: false
#| message: false
#| warning: false
#| label: fig-month
#| fig-cap: Monthly evolution of cart price and delays
#| fig-subcap: 
#|  - "Monthly cart price"
#|  - "Monthly delivery delay"
#| layout-ncol: 2
#| fig-height: 4
df_join |> 
  mutate(
    monthly_date =  floor_date(   # make new column, week of onset
    order_date,
    unit = "month")) |> 
  
  summarize(
    cart_price = median(cart_price_in_cp),
    .by = c(monthly_date, area)
  ) |> 
  ggplot(
    aes(monthly_date, cart_price)
  ) + geom_line() +
  facet_wrap(~area)



df_join |> 
  mutate(
    
    delay = lubridate::date(delivery_date) - lubridate::date(order_date),
    monthly_date =  floor_date(   # make new column, week of onset
    order_date,
    unit = "month")) |> 
  
  summarize(
    delay = median(delay),
    .by = c(monthly_date, area)
  ) |> 
  ggplot(
    aes(monthly_date, delay)
  ) + geom_line() +
  facet_wrap(~area)
  
```

The order and delivery date variable, from which the delivery delay was computed. The evolution of of median cart prices per month is comparable across the different regions but for the Underdark region which shows more of an erratic behavior. Although all regions observed a peak in median delivery delay time around the end of 2021, this is more marked in the Underdark and West regions (@fig-month-2)

```{r}
#| echo: false
#| label: fig-products
#| layout: [[45, -6, 45], [47 , -6, 47]]
#| fig-height: 5
#| fig-cap: Product details
#| fig-subcap: 
#|  - "product type"
#|  - "product subtype"
#|  - "brand names"
#|  - "business line leaders"
products |> 
  mutate(type = stri_trans_totitle(type, type = "sentence") |> fct_infreq() |> fct_rev()) |> 
  ggplot(aes(x = type)) +
  geom_bar() +
  coord_flip() +
  labs(
    x = NULL,
    y = "Frequency"
  ) 

# product subtype
products |> 
  mutate(
    subtype = subtype |> str_to_lower() |> fct_infreq() |> fct_rev()
  ) |> 
  
  ggplot(aes(x = subtype)) +
  geom_bar() +
  coord_flip() +
  labs(
    x = NULL,
    y = "Frequency"
  ) 
# Brand names
products |> 
  mutate(
    brand_name = brand_name  |> fct_infreq() |> fct_rev()
  ) |> 
  
  ggplot(aes(x = brand_name)) +
  geom_bar() +
  coord_flip() +
  labs(
    x = NULL,
    y = "Frequency"
  ) +
  theme_minimal()
# Products business line leaders
products |> 
  mutate(
    products_business_line_leader = products_business_line_leader  |> fct_infreq() |> fct_rev()
  ) |> 
  
  ggplot(aes(x = products_business_line_leader)) +
  geom_bar() +
  coord_flip() +
  labs(
    x = NULL,
    y = "Frequency"
  ) 
```

The product type, sub type are all categorical nominal variables @fig-products-1 and @fig-products-2 shows their respective distribution. The popularity of four out of the 6 brand names is fairly similar, contrary to Arcantic and Mythal which are not very popular brand names (@fig-products-3). Bubles are the most common business line leaders @fig-products-4

```{r}
# write_csv(df_join, "data/df_dead.csv")
```

```{r}
df_chart1 <- df_join |> 
  select(type, order_date, area, revenue = cart_price_in_cp,business_unit, quantity = quantities) |> 
  mutate(
    year = lubridate::year(order_date),
    week = lubridate::week(order_date),
    month = lubridate::month(order_date),
    quantity = as.integer(quantity)
  ) |> 
  mutate(across(where(is.character), str_to_sentence)) |> 
  mutate(
    # standardize week to 52 per year
    week = ifelse(week > 52, 52, week )
  )
  
```

```{r}
df_chart1 |> filter(week > 52)
```

```{r}
revenue_by_type <- df_chart1 %>%
  group_by(business_unit, week, year, type) %>%
  summarize(
    revenue = sum(revenue, na.rm = TRUE),
    type_avg_revenue = mean(revenue, na.rm = TRUE),
    total_type_quantity = sum(quantity, na.rm = TRUE), .groups = "drop"
  ) 
  



```

```{r}
proportion_revenue_week_area <- df_chart1 %>%
  group_by(business_unit, year, week, type, area) %>%
  summarise(total_revenue_by_type = sum(revenue), .groups = 'drop') %>%
  group_by(business_unit, year, week, area) %>%
  mutate(total_revenue_per_week = sum(total_revenue_by_type)) %>%
  ungroup() %>%
  mutate(proportion = total_revenue_by_type / total_revenue_per_week * 100) |> 
  rename(revenue = total_revenue_by_type)

proportion_revenue_week <- df_chart1 %>%
  group_by(business_unit, year, week, type) %>%
  summarise(total_revenue_by_type = sum(revenue), .groups = 'drop') %>%
  group_by(business_unit, year, week) %>%
  mutate(total_revenue_per_week = sum(total_revenue_by_type)) %>%
  ungroup() %>%
  mutate(proportion = total_revenue_by_type / total_revenue_per_week * 100) |> 
  rename(revenue = total_revenue_by_type) |> 
  mutate(area = "Overall")

#
df_chart1 |> 
  group_by(business_unit, year, area) |> 
  summarize(total = sum(revenue), .groups = "drop") |> 
  bind_rows(
    df_chart1 |> 
  group_by(business_unit, year) |> 
  summarize(total = sum(revenue), .groups = "drop") |> 
  mutate(area = "Overall")
    
  )
  


all_revenue_week <- bind_rows(proportion_revenue_week, proportion_revenue_week_area)
  write(toJSON(all_revenue_week, pretty = TRUE), "trend_weekly.json")
```

```{r}
proportion_revenue_month_area <- df_chart1 %>%
  group_by(business_unit, year, month, type, area) %>%
  summarise(total_revenue_by_type = sum(revenue), .groups = 'drop') %>%
  group_by(business_unit, year, month, area) %>%
  mutate(total_revenue_per_month = sum(total_revenue_by_type)) %>%
  ungroup() %>%
  mutate(proportion = total_revenue_by_type / total_revenue_per_month) |> 
  rename(revenue = total_revenue_by_type)

proportion_revenue_month <- df_chart1 %>%
  group_by(business_unit, year, month, type) %>%
  summarise(total_revenue_by_type = sum(revenue), .groups = 'drop') %>%
  group_by(business_unit, year, month) %>%
  mutate(total_revenue_per_month = sum(total_revenue_by_type * 100)) %>%
  ungroup() %>%
  mutate(proportion = total_revenue_by_type / total_revenue_per_month * 100) |> 
  rename(revenue = total_revenue_by_type) |> 
  mutate(area = "Overall")


all_revenue_month <- bind_rows(proportion_revenue_month, proportion_revenue_month_area)
write(toJSON(all_revenue_month, pretty = TRUE), "trend_monthly.json")
```

```{r}

```

```{r}

```

```{r}
revenue_by_week <- df_chart1 |> 
  group_by(business_unit, year, week) |> 
  summarize(total_weekly_revenue = sum(revenue, na.rm = TRUE),
            avg_revenue_weekly = mean(revenue, na.rm = TRUE),
            .groups = "drop")
```

```{r}
yearly_revenue <- revenue_by_type |> 
  
  summarize(total = sum(revenue), .by = c(year, business_unit))

```

```{r}
df_ch1 <- revenue_by_type |> 
  left_join(revenue_by_week, join_by(business_unit, year, week)) |> 
  mutate(percentage = (revenue/total_weekly_revenue) * 100) |> 
  # mutate(across(where(is.character), str_to_sentence)) |> 
  arrange(week, business_unit) |> 
  left_join(yearly_revenue, join_by(year, business_unit))
```

```{r}
yearly_revenue <- df_ch1 |> 
  
  summarize(total = sum(revenue), .by = c(year, business_unit))


```

### Get monthly summaries

```{r}
revenue_by_type <- df_chart1 %>%
  group_by(business_unit, month, year, type, area) %>%
  summarize(
    revenue = sum(revenue, na.rm = TRUE),
    type_avg_revenue = mean(revenue, na.rm = TRUE),
    total_type_quantity = sum(quantity, na.rm = TRUE), .groups = "drop"
  ) 
```

```{r}
revenue_by_month <- df_chart1 |> 
  group_by(business_unit, year, month, area) |> 
  summarize(total_monthly_revenue = sum(revenue, na.rm = TRUE),
            avg_revenue_monthly = mean(revenue, na.rm = TRUE),
            .groups = "drop")
```

```{r}
df_month <- revenue_by_type |> 
  left_join(revenue_by_month, join_by(business_unit, year, month, area)) |> 
  mutate(percentage = (revenue/total_monthly_revenue) * 100) 
  # # mutate(across(where(is.character), str_to_sentence)) |> 
  # # arrange(week, business_unit) |> 
  # left_join(yearly_revenue, join_by(year, business_unit))
```

```{r}
df_month
```

```{r}
json_data_week <- toJSON(df_ch1, pretty = TRUE)
write(json_data_week, "weekly_revenue.json")
```

```{r}
json_data_month <-  toJSON(df_month, pretty = TRUE)
write(json_data_month, "trend_month.json")
```

```{r}
regional_revenues <- df_join |> 
  select(type, order_date, region, revenue = cart_price_in_cp,account_name, account_type, delay) |> 
  mutate(
    year = lubridate::year(order_date),
    week = lubridate::week(order_date),
    month = lubridate::month(order_date)
  ) |> 

  group_by(region, account_type, year, month) |> 
  summarize(revenue = sum(revenue), .groups = "drop") |> 
  filter(year == 2023)
  
```

```{r}
regional_revenues
```

```{r}
json_regional_revenue <- toJSON(regional_revenues, pretty = TRUE)
write(json_regional_revenue, "regional_revenue.json")
```

```{r}
df_delays <- df_join |> 
  group_by(year, type, month_name) |> 
  summarize(delay = parse_number(as.character(round(mean(delay, na.rm = TRUE)))), .groups = "drop") 
  # filter(year == 2020) |> 
  # rename(type = brand_name)
  
df_delays
```

```{r}
write(toJSON(df_delays, pretty = TRUE), "delays.json")
```

```{r}
df_join |> 
  select(products, order_id) |> 
  mutate(
    products = str_trim(str_to_sentence(products)),
    order_id = as.character(order_id)
  ) |> 
  head(250) |> 
write_csv("df_dead.csv")

```

```{python}
import pandas as pd
from itertools import combinations
from collections import defaultdict

# Load the data
data = pd.read_csv('df_dead.csv')

# Prepare the data for pairing
# Group by OrderID and collect products
grouped = data.groupby('order_id')['products'].apply(list)

# Function to count pairs
def count_pairs(items):
    # Create a sorted combination of items to ensure consistency (A, B) == (B, A)
    for item_pair in combinations(sorted(items), 2):
        yield item_pair

# Counter for pairs
pair_counter = defaultdict(int)

# Iterate over each order and update the count of each pair
for products in grouped:
    for pair in count_pairs(products):
        pair_counter[pair] += 1

# Create a DataFrame from the pair counter
pair_df = pd.DataFrame(((pair[0], pair[1], count) for pair, count in pair_counter.items()),
                       columns=['source', 'target', 'value'])

# Save the pair DataFrame to a new CSV (optional)
pair_df.to_csv('product_pairs.csv', index=False)

# If you want to convert this to a JSON format suitable for an arc diagram:
nodes = list(set(pair_df['source']).union(set(pair_df['target'])))
nodes = [{'id': node} for node in nodes]
links = pair_df.to_dict(orient='records')
result = {'nodes': nodes, 'links': links}

# Save to JSON (you might need additional steps depending on your visualization library)
import json
with open('arc_diagram_data.json', 'w') as f:
    json.dump(result, f, indent = 4)

```

```{python}
import pandas as pd
import networkx as nx
import matplotlib.pyplot as plt
data = pd.read_csv("data/products_basket_analysis.csv")
# Initialize a directed graph
G = nx.DiGraph()

# Add nodes and edges from the dataframe
for idx, row in data.iterrows():
    # Adding nodes for each unique product
    G.add_node(row['antecedent_name'], area=row['Area'])
    G.add_node(row['consequent_name_1'], area=row['Area'])
    G.add_node(row['consequent_name_2'], area=row['Area'])
    
    # Adding edges with the value as weight
    G.add_edge(row['antecedent_name'], row['consequent_name_1'], weight=row['value'])
    G.add_edge(row['antecedent_name'], row['consequent_name_2'], weight=row['value'])

# Draw the network graph
plt.figure(figsize=(12, 12))
pos = nx.spring_layout(G)  # positions for all nodes
nx.draw_networkx_nodes(G, pos, node_size=700)
nx.draw_networkx_edges(G, pos, width=1)
nx.draw_networkx_labels(G, pos, font_size=8, font_family='sans-serif')

# Show edge weights
edge_labels = nx.get_edge_attributes(G, 'weight')
nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels)

plt.title('Network Diagram of Products and Relationships')
plt.axis('off')  # Turn off the axis
plt.show()


```

```{r}
# Load necessary libraries
library(dplyr)
library(jsonlite)

# Read the data from CSV
data <- read.csv("data/products_basket_analysis.csv")

# Generate nodes data
nodes <- data %>%
  select(antecedent_name, Area) %>%
  distinct() %>%
  rename(id = antecedent_name, group = Area) %>%
  bind_rows(
    data %>%
      select(consequent_name_1, Area) %>%
      distinct() %>%
      rename(id = consequent_name_1, group = Area),
    data %>%
      select(consequent_name_2, Area) %>%
      distinct() %>%
      rename(id = consequent_name_2, group = Area)
  ) %>%
  distinct()

# Generate links data
links <- data %>%
  select(antecedent_name, consequent_name_1, value) %>%
  rename(source = antecedent_name, target = consequent_name_1) %>%
  bind_rows(
    data %>%
      select(antecedent_name, consequent_name_2, value) %>%
      rename(source = antecedent_name, target = consequent_name_2)
  )

# Combine nodes and links into a list
graph <- list(
  nodes = nodes,
  links = links
)

# Convert the graph data to JSON
json_data <- toJSON(graph, pretty = TRUE)

# Optionally, write the JSON to a file
write(json_data, "network_data.json")

```

```{r}

all_revenue_week |> 
  filter(week>52)
```
